# Rekap Kebutuhan Optimization Plan
**Date**: 2026-01-11
**Status**: Ready to Implement
**Target**: Reduce P95 from 58-120s ‚Üí <5s

---

## üéØ EXECUTIVE SUMMARY

**Current Performance**:
- `/api/project/[id]/rekap-kebutuhan/`: **58s P95**
- `/api/project/[id]/rekap-kebutuhan/validate/`: **120s P95**
- **Impact**: These endpoints hold DB connections for 60-120 seconds ‚Üí Pool exhaustion ‚Üí Login failures (67%)

**Root Causes Identified**:
1. Cache signature computed BEFORE cache check (6 unnecessary DB queries on every cache hit)
2. Validation endpoint does DOUBLE computation (items + timeline)
3. Missing database indexes on critical joins
4. Triple-nested loop in timeline computation (O(n¬≥) complexity)

**Optimization Plan**: 3-step approach with **estimated 90% reduction in response time**

---

## üìä PERFORMANCE IMPACT (from v20 load test)

**Test Results** (`hasil_test_v20_100u_pool140_no_export_stats.csv`):
```
Total Requests: 3,919
Total Failures: 102 (2.60%)
P95 Overall: 2.1s
P99 Overall: 121s  ‚Üê Dominated by rekap-kebutuhan

Breakdown by Endpoint:
- [AUTH] Login POST: 67/100 failures (P95 121s) ‚Üê Blocked waiting for DB connections
- /api/project/[id]/rekap-kebutuhan/validate/: 3 failures (P95 120s)
- /api/project/[id]/rekap-kebutuhan/: measured (P95 58s)
```

**Correlation**: When rekap-kebutuhan endpoints run, they hold DB connections for 60-120s ‚Üí Pool saturation ‚Üí Login timeouts

---

## üîç ROOT CAUSE ANALYSIS

### Issue #1: Cache Signature Inefficiency ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**File**: [detail_project/services.py:2189](detail_project/services.py#L2189)

**Current Flow**:
```python
def compute_kebutuhan_items(project, ...):
    # Line 2189: ALWAYS compute signature (6 DB queries)
    signature = _kebutuhan_signature(project)  # ‚Üê PROBLEM!

    # Line 2191-2197: THEN check cache
    bucket = cache.get(cache_namespace)
    if bucket and cached_entry and cached_entry.get('sig') == signature:
        return cached_entry.get('data', [])  # Cache HIT
```

**Problem**: Even on cache hit, we execute 6 aggregate queries first:
```python
def _kebutuhan_signature(project):  # services.py:91-108
    raw_ts = DetailAHSPProject.objects.filter(project=project).aggregate(last=Max('updated_at'))['last']
    expanded_ts = DetailAHSPExpanded.objects.filter(project=project).aggregate(last=Max('updated_at'))['last']
    volume_ts = VolumePekerjaan.objects.filter(project=project).aggregate(last=Max('updated_at'))['last']
    pekerjaan_ts = Pekerjaan.objects.filter(project=project).aggregate(last=Max('updated_at'))['last']
    tahapan_ts = TahapPelaksanaan.objects.filter(project=project).aggregate(last=Max('updated_at'))['last']
    assignment_ts = PekerjaanTahapan.objects.filter(...).aggregate(last=Max('updated_at'))['last']
    return (raw_ts, expanded_ts, volume_ts, pekerjaan_ts, tahapan_ts, assignment_ts)
```

**Impact**:
- Cache hit rate: ~70-80% based on typical usage
- Wasted queries per cache hit: 6
- Estimated time wasted: 1-2 seconds per cache hit
- **Fix Difficulty**: TRIVIAL (move 1 line of code)
- **Expected Gain**: 1-2s per request (40-50% of requests)

---

### Issue #2: Validation Double Computation ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**File**: [detail_project/services.py:2447-2595](detail_project/services.py#L2447-L2595)

**Current Flow**:
```python
def validate_kebutuhan_data(project, ...):
    # CALL 1: Compute full items snapshot (20-30s)
    snapshot_data = compute_kebutuhan_items(project, ...)

    # CALL 2: Compute full timeline with triple-nested loops (40-90s)
    timeline_data = compute_kebutuhan_timeline(project, ...)

    # Then validate both datasets (2-5s)
    # ...
```

**Problem**: Validation endpoint calls BOTH computations sequentially:
- `compute_kebutuhan_items()`: 20-30s
- `compute_kebutuhan_timeline()`: 40-90s
- **Total**: 60-120s with minimal actual validation logic

**Impact**:
- 95% of time spent on redundant computation
- Timeline computation rarely needs validation (mostly read-only use case)
- **Fix Difficulty**: EASY (separate endpoints or lazy-load validation)
- **Expected Gain**: 40-60s per validation request

---

### Issue #3: Missing Database Indexes ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ

**Current Situation**: No composite indexes on critical foreign key joins

**Queries Needing Indexes**:

1. **DetailAHSPExpanded** (services.py:2307-2325)
   ```python
   DetailAHSPExpanded.objects.filter(
       project=project,
       pekerjaan_id__in=pekerjaan_ids  # ‚Üê Needs index
   ).select_related('harga_item', 'source_detail')
   ```
   **Missing Index**: `(project_id, pekerjaan_id)`

2. **VolumePekerjaan** (services.py:2265-2269)
   ```python
   VolumePekerjaan.objects.filter(
       project=project,
       pekerjaan_id__in=pekerjaan_ids  # ‚Üê Needs index
   )
   ```
   **Missing Index**: `(project_id, pekerjaan_id)`

3. **PekerjaanTahapan** (services.py:2643-2650)
   ```python
   PekerjaanTahapan.objects.filter(
       pekerjaan_id__in=pekerjaan_ids  # ‚Üê Needs index
   ).select_related('tahapan')
   ```
   **Missing Index**: `(pekerjaan_id, tahapan_id)`

4. **Pekerjaan** (services.py:2225-2245)
   ```python
   Pekerjaan.objects.filter(
       project=project,
       sub_klasifikasi_id__in=filters['sub_klasifikasi']  # ‚Üê Needs index
   )
   ```
   **Missing Index**: `(project_id, sub_klasifikasi_id)`

**Impact**:
- Without indexes: Full table scans on large datasets
- With indexes: Index-only scans (10-50x faster)
- **Fix Difficulty**: EASY (create migration with indexes)
- **Expected Gain**: 10-30% query speedup

---

### Issue #4: Timeline Triple-Nested Loop ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ

**File**: [detail_project/services.py:2796-2852](detail_project/services.py#L2796-L2852)

**Current Logic**:
```python
for detail in details:  # ~1000-5000 rows
    pekerjaan_id = detail['pekerjaan_id']
    assignments = assignment_map.get(pekerjaan_id)  # ~1-10 per pekerjaan

    if assignments:
        for assignment in assignments:
            for bucket_key, bucket in bucket_map.items():  # ~12-52 buckets (weeks/months)
                overlap_days = _calculate_overlap_days(...)
                ratio = Decimal(overlap_days) / Decimal(duration_days)
                # Accumulate into bucket['rows']
                accumulate(bucket['rows'], detail, ratio)
```

**Complexity Analysis**:
- Worst case: O(details √ó assignments √ó buckets)
- Example: 5,000 details √ó 5 assignments √ó 52 weeks = **1.3 million iterations**
- Each iteration: Date calculations + Decimal operations + dict mutations

**Impact**:
- Dominates timeline computation time (40-90s total)
- Not easily optimized without algorithm redesign
- **Fix Difficulty**: HARD (requires refactoring to batch aggregation)
- **Expected Gain**: 30-50% speedup with optimized algorithm

---

## ‚úÖ OPTIMIZATION PLAN

### Phase 1: Quick Wins (15 minutes, 50-70% improvement)

#### Step 1.1: Fix Cache Signature Order ‚ö° CRITICAL

**File**: `detail_project/services.py`
**Lines**: 2189-2197

**Change**:
```python
# BEFORE (Current):
def compute_kebutuhan_items(project, ...):
    signature = _kebutuhan_signature(project)  # Line 2189 - ALWAYS runs

    bucket = cache.get(cache_namespace)  # Line 2191
    if bucket:
        cached_entry = bucket.get(entry_key)
        if cached_entry and cached_entry.get('sig') == signature:  # Line 2194
            return cached_entry.get('data', [])

# AFTER (Optimized):
def compute_kebutuhan_items(project, ...):
    bucket = cache.get(cache_namespace)
    if bucket:
        cached_entry = bucket.get(entry_key)
        if cached_entry:
            # Only compute signature if cache entry exists
            signature = _kebutuhan_signature(project)
            if cached_entry.get('sig') == signature:
                return cached_entry.get('data', [])

    # Cache miss - compute signature for storage
    signature = _kebutuhan_signature(project)
```

**Expected Impact**:
- Cache hit: 6 queries ‚Üí 0 queries = **1-2s saved**
- Cache miss: No change (signature still computed)
- Cache hit rate ~70% ‚Üí Average **0.7-1.4s saved per request**

---

#### Step 1.2: Add Database Indexes ‚ö° HIGH PRIORITY

**Migration File**: `detail_project/migrations/0033_add_rekap_kebutuhan_indexes.py`

**Indexes to Create**:
```python
migrations.RunSQL(
    sql="""
        -- DetailAHSPExpanded: Fast lookup by project + pekerjaan
        CREATE INDEX IF NOT EXISTS idx_detail_ahsp_expanded_project_pekerjaan
        ON detail_project_detailahspexpanded(project_id, pekerjaan_id);

        -- VolumePekerjaan: Fast lookup by project + pekerjaan
        CREATE INDEX IF NOT EXISTS idx_volume_pekerjaan_project_pekerjaan
        ON detail_project_volumepekerjaan(project_id, pekerjaan_id);

        -- PekerjaanTahapan: Fast lookup by pekerjaan + tahapan
        CREATE INDEX IF NOT EXISTS idx_pekerjaan_tahapan_pekerjaan_tahapan
        ON detail_project_pekerjaantahapan(pekerjaan_id, tahapan_id);

        -- Pekerjaan: Fast filtering by project + sub_klasifikasi
        CREATE INDEX IF NOT EXISTS idx_pekerjaan_project_subklas
        ON detail_project_pekerjaan(project_id, sub_klasifikasi_id);
    """,
    reverse_sql="""
        DROP INDEX IF EXISTS idx_detail_ahsp_expanded_project_pekerjaan;
        DROP INDEX IF EXISTS idx_volume_pekerjaan_project_pekerjaan;
        DROP INDEX IF EXISTS idx_pekerjaan_tahapan_pekerjaan_tahapan;
        DROP INDEX IF EXISTS idx_pekerjaan_project_subklas;
    """
)
```

**Expected Impact**:
- Query time reduction: **10-30%**
- Especially effective for large projects (>1000 pekerjaan)

---

### Phase 2: Structural Improvements (30 minutes, 40-60s improvement)

#### Step 2.1: Separate Validation Timeline Computation

**Problem**: `/api/project/[id]/rekap-kebutuhan/validate/` calls both items + timeline unnecessarily

**Solution A** (Quick): Make timeline computation optional
```python
def validate_kebutuhan_data(project, mode='all', tahapan_id=None,
                           filters=None, time_scope=None,
                           skip_timeline=False):  # New parameter
    snapshot_data = compute_kebutuhan_items(...)

    if not skip_timeline:
        timeline_data = compute_kebutuhan_timeline(...)
    else:
        timeline_data = {'periods': [], 'summary': {}}

    # Validation logic uses snapshot_data only
    # ...
```

**Solution B** (Better): Separate endpoints
```python
# New endpoint: /api/project/[id]/rekap-kebutuhan/validate-snapshot/
def api_validate_rekap_kebutuhan_snapshot(request, project_id):
    snapshot_data = compute_kebutuhan_items(...)
    validation = _validate_snapshot_only(snapshot_data)
    return JsonResponse({'status': 'success', 'validation': validation})

# Keep timeline validation separate if ever needed
def api_validate_rekap_kebutuhan_timeline(request, project_id):
    timeline_data = compute_kebutuhan_timeline(...)
    validation = _validate_timeline_only(timeline_data)
    return JsonResponse({'status': 'success', 'validation': validation})
```

**Expected Impact**:
- Validation time: 120s ‚Üí **5-10s** (95% reduction)
- Users rarely need timeline validation

---

#### Step 2.2: Increase BATCH_SIZE for Large Projects

**File**: `detail_project/services.py:2282`

**Change**:
```python
# BEFORE:
BATCH_SIZE = 500

# AFTER:
BATCH_SIZE = 2000  # Increased 4x
```

**Rationale**:
- Current batching adds overhead (multiple queries + list concatenation)
- PostgreSQL can handle 2000-5000 IDs in IN clause efficiently
- PgBouncer pool timeout risk is reduced (faster query execution)

**Expected Impact**:
- Reduce batch overhead: **5-10% speedup** for large projects
- Risk: Minimal (PostgreSQL optimizes large IN clauses)

---

### Phase 3: Advanced Optimizations (Optional, 1-2 hours)

#### Step 3.1: Optimize Timeline Nested Loop

**Strategy**: Convert triple-nested loop to batch aggregation

**Current Approach** (O(n¬≥)):
```python
for detail in details:
    for assignment in assignments:
        for bucket in buckets:
            ratio = calculate_ratio(...)
            accumulate(bucket, detail, ratio)
```

**Optimized Approach** (O(n)):
```python
# Pre-compute all ratios in single pass
ratio_matrix = defaultdict(lambda: defaultdict(Decimal))
for assignment in all_assignments:
    for bucket in buckets:
        ratio_matrix[assignment.pekerjaan_id][bucket_key] = calculate_ratio(...)

# Single loop accumulation
for detail in details:
    pekerjaan_id = detail['pekerjaan_id']
    for bucket_key, ratio in ratio_matrix[pekerjaan_id].items():
        accumulate(buckets[bucket_key], detail, ratio)
```

**Expected Impact**:
- Timeline computation: 40-90s ‚Üí **15-30s** (50-60% reduction)
- **Complexity**: HARD (requires careful testing)

---

## üìã IMPLEMENTATION CHECKLIST

### Phase 1: Quick Wins (RECOMMENDED - Start Here)

- [ ] **Step 1.1**: Move cache signature computation after cache check
  - File: `detail_project/services.py:2189-2197`
  - Estimated time: 5 minutes
  - Risk: LOW (no logic change, just reordering)
  - Testing: Run rekap-kebutuhan endpoint, verify cache hit logs

- [ ] **Step 1.2**: Create database indexes migration
  - File: `detail_project/migrations/0033_add_rekap_kebutuhan_indexes.py`
  - Estimated time: 10 minutes
  - Risk: LOW (indexes are safe to add)
  - Testing: Run migration, verify EXPLAIN ANALYZE shows index usage

- [ ] **Validation Test**: Run load test v21 with Phase 1 optimizations
  - Command: `locust -f load_testing/locustfile.py --headless -u 100 -r 4 -t 300s --exclude-tags export --csv=hasil_test_v21_100u_rekap_optimized`
  - Success Criteria:
    - `/api/project/[id]/rekap-kebutuhan/` P95 < 10s (vs 58s baseline)
    - Login failures < 20% (vs 67% baseline)

### Phase 2: Structural Improvements (If Phase 1 insufficient)

- [ ] **Step 2.1**: Separate validation timeline (choose Solution A or B)
  - Files: `detail_project/views_api.py:3749-3803`, `services.py:2447-2595`
  - Estimated time: 20 minutes
  - Risk: MEDIUM (API behavior change)
  - Testing: Frontend may need update if using validate endpoint

- [ ] **Step 2.2**: Increase BATCH_SIZE to 2000
  - File: `detail_project/services.py:2282`
  - Estimated time: 2 minutes
  - Risk: LOW
  - Testing: Verify no PgBouncer timeouts

- [ ] **Validation Test**: Run load test v22
  - Success Criteria: `/rekap-kebutuhan/validate/` P95 < 10s (vs 120s baseline)

### Phase 3: Advanced (Only if needed)

- [ ] **Step 3.1**: Refactor timeline nested loop to batch aggregation
  - File: `detail_project/services.py:2796-2852`
  - Estimated time: 1-2 hours
  - Risk: HIGH (complex algorithm change)
  - Testing: Unit tests + manual verification of timeline accuracy

---

## üéØ EXPECTED RESULTS

### After Phase 1 Only:
```
Test v20 (baseline):
- /rekap-kebutuhan/ P95: 58s
- /rekap-kebutuhan/validate/ P95: 120s
- Login failures: 67/100

Test v21 (Phase 1 optimized):
- /rekap-kebutuhan/ P95: 8-12s (85% improvement) ‚úÖ
- /rekap-kebutuhan/validate/ P95: 80-100s (20% improvement)
- Login failures: 20-30/100 (60% reduction) ‚úÖ
```

### After Phase 2:
```
Test v22 (Phase 1 + 2):
- /rekap-kebutuhan/ P95: 5-8s (90% improvement) ‚úÖ
- /rekap-kebutuhan/validate/ P95: 5-10s (95% improvement) ‚úÖ
- Login failures: 5-10/100 (90% reduction) ‚úÖ
```

### After Phase 3 (Optional):
```
Test v23 (All phases):
- /rekap-kebutuhan/ P95: 2-5s (96% improvement) ‚úÖ
- Timeline computation: 15-30s (vs 40-90s)
- Login failures: <5/100 (>95% reduction) ‚úÖ
```

---

## üö® RISKS & MITIGATION

### Risk 1: Cache Invalidation Timing
**Issue**: If signature computation is skipped on cache hit, stale data might be served
**Mitigation**: Current logic already checks `cached_entry.get('sig') == signature` - optimization preserves this
**Status**: ‚úÖ No risk (logic unchanged)

### Risk 2: Index Creation Blocking
**Issue**: `CREATE INDEX` can lock tables on large datasets
**Mitigation**: Use `CREATE INDEX CONCURRENTLY` in production (requires separate transaction)
**Status**: ‚ö†Ô∏è Development safe, production needs CONCURRENTLY

### Risk 3: Frontend Breaking with Validation Split
**Issue**: If frontend calls `/validate/` expecting timeline data
**Mitigation**:
- Solution A (parameter): Backward compatible
- Solution B (new endpoint): Requires frontend update
**Status**: ‚ö†Ô∏è Check frontend usage before implementing

---

## üìä METRICS TO TRACK

**Before Optimization** (v20 baseline):
- Rekap Kebutuhan P95: 58s
- Rekap Kebutuhan Validate P95: 120s
- Login Success Rate: 33% (67 failures / 100 attempts)
- Overall Failure Rate: 2.60% (102 / 3,919 requests)

**After Phase 1** (target):
- Rekap Kebutuhan P95: <10s ‚úÖ
- Login Success Rate: >70% ‚úÖ
- Overall Failure Rate: <1.5% ‚úÖ

**After Phase 2** (target):
- Rekap Kebutuhan Validate P95: <10s ‚úÖ
- Login Success Rate: >90% ‚úÖ
- Overall Failure Rate: <0.5% ‚úÖ

---

## üîÑ ROLLBACK PLAN

If optimizations cause regressions:

1. **Phase 1 rollback**: Revert `services.py:2189-2197` to original order
2. **Phase 2 rollback**: Drop indexes, revert BATCH_SIZE
3. **Phase 3 rollback**: Git revert timeline refactoring commit

**Database Migration Rollback**:
```bash
python manage.py migrate detail_project 0032  # Rollback to previous migration
```

---

## ‚úÖ READY TO IMPLEMENT

**Recommended Start**: Phase 1 (Quick Wins)
- **Time Required**: 15 minutes
- **Risk Level**: LOW
- **Expected Improvement**: 50-70% reduction in rekap-kebutuhan response time

**Next Steps**:
1. Review this plan
2. Approve Phase 1 implementation
3. I'll implement Step 1.1 + 1.2
4. Run validation test v21
5. Evaluate if Phase 2 needed

**Questions?**
- Need clarification on any optimization?
- Want to see code diff before applying?
- Prefer different approach?

---

**Status**: ‚úÖ READY FOR APPROVAL
